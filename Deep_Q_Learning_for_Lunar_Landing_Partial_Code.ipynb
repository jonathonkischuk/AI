{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonathonkischuk/AI/blob/main/Deep_Q_Learning_for_Lunar_Landing_Partial_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbZcI9ZXHl3a"
      },
      "source": [
        "# Deep Q-Learning for Lunar Landing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8yPRjteXgPb"
      },
      "source": [
        "## Part 0 - Installing the required packages and importing the libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slEm5teGWjWU"
      },
      "source": [
        "### Installing Gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dbnq3XpoKa_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daad80ea-85c9-4085-f7fc-3dfe8262f48b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (4.66.1)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (6.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2023.11.17)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=850728447c78d0cb429ba858a51fc103f6e25006a77ffbf82810ce196017f46e\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 shimmy-0.2.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (927 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 121658 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.1.1.post1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349118 sha256=da044529d60050faf7ee562280425875156bd78ce743e8e1e73b811f093a5316\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.1.1.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brqiMN3UW9T9"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mZaKXP_aMl9O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from collections import deque, namedtuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzlDKXvkXzGI"
      },
      "source": [
        "## Part 1 - Building the AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtG6Zc83YYy3"
      },
      "source": [
        "### Creating the architecture of the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "  def __init__(self, state_size, action_size, seed=42):\n",
        "    super(Network, self).__init__()\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    self.fc1 = nn.Linear(state_size, 64)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.fc1(state)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    return self.fc3(x)"
      ],
      "metadata": {
        "id": "buXsawglJEcG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxVrBnFWZKb1"
      },
      "source": [
        "## Part 2 - Training the AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T364fz9qZb2j"
      },
      "source": [
        "### Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make('LunarLander-v2')\n",
        "state_shape = env.observation_space.shape\n",
        "state_size = env.observation_space.shape[0]\n",
        "number_actions = env.action_space.n\n",
        "print('State Shape: ', state_shape)\n",
        "print('State Size: ', state_size)\n",
        "print('Number of Actions: ', number_actions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei-iF4M1sTXa",
        "outputId": "095ec893-f8ee-4600-c3cc-e15113ccd10a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State Shape:  (8,)\n",
            "State Size:  8\n",
            "Number of Actions:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_dZmOIvZgj-"
      },
      "source": [
        "### Initializing the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4\n",
        "minibatch_size = 100\n",
        "discount_factor = 0.99\n",
        "replay_buffer_size = int(1e5)\n",
        "interpolation_parameter = 1e-3"
      ],
      "metadata": {
        "id": "Taz0SoI4yIhV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hD_Vs-bYnip"
      },
      "source": [
        "### Implementing Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.capacity = capacity\n",
        "    self.memory = []\n",
        "\n",
        "\n",
        "  def push(self, event):\n",
        "    self.memory.append(event)\n",
        "    if len(self.memory) > self.capacity:\n",
        "      del self.memory[0]\n",
        "\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    experiences = random.sample(self.memory, k=batch_size)\n",
        "    states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
        "    actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).float().to(self.device)\n",
        "    rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
        "    next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
        "    dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None])).float().to(self.device)\n",
        "    return states, actions, rewards, next_states, dones\n",
        ""
      ],
      "metadata": {
        "id": "vVq09lBT3ESC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmEkbFbUY6Jt"
      },
      "source": [
        "### Implementing the DQN class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.local_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    self.target_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr=learning_rate)\n",
        "    self.memory = ReplayMemory(replay_buffer_size)\n",
        "    self.t_step = 0\n",
        "\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    self.memory.push((state, action, reward, next_state, done))\n",
        "    self.t_step = (self.t_step + 1) % 4\n",
        "    if self.t_step == 0:\n",
        "      experiences = self.memory.sample(100)\n",
        "      self.learn(experiences, discount_factor)\n",
        "\n",
        "\n",
        "  def act(self, state, epsilon=0.):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "    self.local_qnetwork.eval()\n",
        "    with torch.no_grad():\n",
        "        action_values = self.local_qnetwork(state)\n",
        "    self.local_qnetwork.train()\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arrange(self.action_size))\n",
        "\n",
        "\n",
        "  def learn(self, experiences, discount_factor):\n",
        "    states, next_states, actions, rewards, dones = experiences\n",
        "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    q_targets = rewards + (discount_factor + next_q_targets * (1 - dones))\n",
        "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "    loss = F.mse_loss(q_expected, q_targets)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n",
        "\n",
        "\n",
        "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)"
      ],
      "metadata": {
        "id": "ZDBysglKdrCa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1tZElccZmf6"
      },
      "source": [
        "### Initializing the DQN agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(state_size, number_actions)"
      ],
      "metadata": {
        "id": "3RvaTTAxjOPG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8v0PtUfaVQp"
      },
      "source": [
        "### Training the DQN agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_episodes = 2000\n",
        "max_number_timesteps = 1000\n",
        "epsilon_start_value = 1.0\n",
        "epsilon_end_value = 0.01\n",
        "epsilon_decay_value = 0.995\n",
        "epsilon = epsilon_start_value\n",
        "scores_on_100_epiosodes = deque(maxlen=100)\n",
        "\n"
      ],
      "metadata": {
        "id": "TPf0BlP8pRmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8CNwdOTcCoP"
      },
      "source": [
        "## Part 3 - Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb9nVvU2Okhk"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action.item())\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'LunarLander-v2')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}